{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 13\n",
    "\n",
    "In this homework you are going to build your first classifier for the CIFAR-10 dataset. This dataset contains 10 different classes and you can learn more about it [here](https://www.cs.toronto.edu/~kriz/cifar.html). This homework consists of the following tasks:\n",
    "* Dataset inspection\n",
    "* Building the network\n",
    "* Training\n",
    "* Evaluation\n",
    "\n",
    "At the end, as usual, there will be a couple of questions for you to answer :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Dataset Inspection\n",
    "\n",
    "Load the dataset and make a quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# Mapping from class ID to class name\n",
    "classes = {0:'plane', 1:'car', 2:'bird', 3:'cat', 4:'deer',\n",
    "           5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
    "\n",
    "# Dataset params\n",
    "num_classes = len(classes)\n",
    "size = x_train.shape[1]\n",
    "\n",
    "# Visualize random samples (as a plot with 3x6 samples)\n",
    "for ii in range(18):    \n",
    "    plt.subplot(3,6,ii+1)\n",
    "    # Pick a random sample\n",
    "    idx = np.random.randint(0, len(y_train))\n",
    "    # Show the image and the label\n",
    "    plt.imshow(x_train[idx, ...])\n",
    "    plt.title(classes[int(y_train[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class histogram (you can visualize it if you want). Is the dataset balanced?\n",
    "\n",
    "Hint: You might find [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) tool useful. In any case, it's up to you how you compute the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class histogram\n",
    "from collections import Counter\n",
    "hist = Counter(y_train.flatten())\n",
    "\n",
    "plt.bar(hist.keys(), hist.values()), plt.grid(True)\n",
    "plt.xlabel('Classess'), plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n",
    "\n",
    "In this step, you'll need to prepare the data for training, i.e., you will have to normalize it and encode the labels as one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the Classifier\n",
    "\n",
    "Build the CNN for CIFAR10 classification. For starters, you can use the same network we used in the lesson for the MNIST problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Dropout, BatchNormalization, Activation, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the classifier\n",
    "inputs = Input(shape=x_train[0].shape)\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same')(inputs)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Conv2D(64, kernel_size=(3, 3,), activation=\"relu\", padding='same')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Conv2D(64, kernel_size=(3, 3,), activation=\"relu\", padding='same')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Flatten()(net)\n",
    "net = Dense(64, activation='relu')(net)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(net)\n",
    "\n",
    "# inputs = Input(shape=x_train[0].shape)\n",
    "\n",
    "# def block(inputs, filters, size, drop_out=None):\n",
    "#     global i\n",
    "#     i += 1\n",
    "#     x = Conv2D(filters, size, padding='same', activation='relu')(inputs)\n",
    "#     x = MaxPooling2D((2, 2))(x)\n",
    "#     if drop_out != None:\n",
    "#         x = Dropout(drop_out)(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# net = Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(inputs)\n",
    "# net = BatchNormalization()(net)\n",
    "# net = Activation('relu')(net)\n",
    "\n",
    "# for i in range(5, 10):\n",
    "#     if i<9:\n",
    "#         net = block(net, np.power(2,i), (3,3), drop_out=0.2)\n",
    "#     else:\n",
    "#         net = block(net, np.power(2,i), (3,3))\n",
    "\n",
    "# net = GlobalAveragePooling2D()(net)\n",
    "# net = Dropout(0.3)(net)\n",
    "# outputs = Dense(num_classes, activation=\"softmax\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Show the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training\n",
    "\n",
    "Compile the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history (this cell is complete, nothing to implement here :-) )\n",
    "h = history.history\n",
    "epochs = range(len(h['loss']))\n",
    "\n",
    "plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.subplot(122), plt.plot(epochs, h['accuracy'], '.-',\n",
    "                           epochs, h['val_accuracy'], '.-')\n",
    "plt.grid(True), plt.xlabel('epochs'), plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "print('Train Acc     ', h['accuracy'][-1])\n",
    "print('Validation Acc', h['val_accuracy'][-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation\n",
    "\n",
    "In this step, you have to calculate the accuracies and visualize some random samples. For the evaluation, you are going to use the test split from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the labels and the predictions as sparse values\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print('True', y_true[0:10])\n",
    "print('Pred', y_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the accuracy for each class\n",
    "for class_id, class_name in classes.items():\n",
    "    mask = y_true == class_id\n",
    "    acc = np.sum(y_pred[mask] == class_id)/np.sum(mask)\n",
    "    print(class_name, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall stats\n",
    "ev = model.evaluate(x_test, y_test)\n",
    "print('Test loss  ', ev[0])\n",
    "print('Test metric', ev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random samples\n",
    "for ii in range(15):\n",
    "    # Pick a random sample\n",
    "    idx = np.random.randint(0, len(y_test))\n",
    "    # Show the results\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_test[idx, ...])\n",
    "    plt.title('True: ' + str(classes[y_true[idx]]) + ' | Pred: ' + str(classes[y_pred[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* What is the overall accuracy of the classifier?\n",
    "\n",
    "    Best that i made \n",
    "    * plane 0.801\n",
    "    * car 0.822\n",
    "    * bird 0.631\n",
    "    * cat 0.531\n",
    "    * deer 0.745\n",
    "    * dog 0.625\n",
    "    * frog 0.829\n",
    "    * horse 0.754\n",
    "    * ship 0.825\n",
    "    * truck 0.838\n",
    "\n",
    "    Found in internet\n",
    "\n",
    "    * plane 0.822\n",
    "    * car 0.896\n",
    "    * bird 0.755\n",
    "    * cat 0.703\n",
    "    * deer 0.783\n",
    "    * dog 0.768\n",
    "    * frog 0.887\n",
    "    * horse 0.862\n",
    "    * ship 0.922\n",
    "    * truck 0.899\n",
    "\n",
    "* What modifications would you do in order to improve the classification accuracy?\n",
    "\n",
    "    Best result i received after adding padding and adding extra laeer with convolution.\n",
    "\n",
    "* Make **one** modification (that you think can help) and train the classifier again. Does the accuracy improve?\n",
    "\n",
    "    It is possible to play with the activation functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-mac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4e4f01325c912b79e0f8552eafc8362cdad02ecd1e6192b8db90dbb446990e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
